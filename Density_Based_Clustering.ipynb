{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entire notebook is based on data that is still unpublished and thus completely commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "import math as mt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dispersion function:\n",
    "def dispersion(ClusterData, x = 'x_coord', y = 'y_coord'):\n",
    "    '''\n",
    "    This function calculates the dispersion of a cluster from a pandas dataframe giving the x and y coordinates of .\n",
    "    The dispersion is defined as the standard deviation of the distance between all possible crater pairs.\n",
    "    This method gives meaningul results for clusters with more than 3 craters in a cluster.\n",
    "    It is using the radius of Mars to convert from lat/lon data to metres.\n",
    "\n",
    "    :param ClusterData: Dataframe containing all craters in clusters\n",
    "    :type ClusterData: pandas dataframe\n",
    "    :param x: column name giving the longitude, defaults to 'x_coord'\n",
    "    :type x: str\n",
    "    :param y: column name giving the latitude, defaults to 'y_coord'\n",
    "    :type y: str\n",
    "    '''    \n",
    "    Rmars = 3390000 #radius of Mars in metres\n",
    "    #x and y are the names of the column in ClusterData denoting the x and y coordinates respectively\n",
    "    #Assumes that x and y are in degrees still!\n",
    "    df = ClusterData.copy() #create work copy of database\n",
    "    coord_array = np.array(df[[x, y]]) #create array of xy coordinates for craters in cluster\n",
    "    sep_list = []\n",
    "    for n in range(0, len(coord_array)): #iterating over all craters for separation calculation\n",
    "        for m in range(n+1, len(coord_array)): #calculating seperation ((x2-x1)**2 + (y2 - y1)**2)**0.5 for all combinations\n",
    "            dx= (coord_array[m,0] - coord_array[n,0]) *Rmars*(np.pi/180)*mt.sin(mt.radians(90 - ((coord_array[m,0]+ coord_array[n,0])/2))) #converting to metres based xy coordinates\n",
    "            dy = (coord_array[m,1] - coord_array[n,1]) *Rmars*(np.pi/180)\n",
    "            sep = (dx**2+ dy**2)**0.5\n",
    "            sep_list.append(sep) #adding all separations to list\n",
    "    dispersion = np.std(sep_list) #calculating dispersion as standard deviation\n",
    "    return dispersion, sep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing test data\n",
    "\n",
    "#df0 = pd.read_excel('C:/Users/jae4518/OneDrive - Imperial College London/HiRise_Images_Clusters/ClustersDataSheet/ESP_057927_1620formatted.xlsx')\n",
    "#ID0 = 'ESP_057927_1620' #clear clustering is observed\n",
    "#df1 = pd.read_excel('ESP_059292_2030formatted.xlsx', index_col = [0,1])\n",
    "#ID1 = 'ESP_059292_2030' #clear clustering is observed\n",
    "#df2 = pd.read_excel('ESP_059387_1605formatted.xlsx', index_col = [0,1])\n",
    "#ID2 = 'ESP_059387_1605' #possible clustering\n",
    "#df3 = pd.read_excel('ESP_059451_1950formatted.xlsx', index_col = [0,1])\n",
    "#ID3 = 'ESP_059451_1950' #no clustering observed\n",
    "#param = pd.read_excel('C:/Users/jae4518/OneDrive - Imperial College London/HiRise_Images_Clusters/Analysis/NewClustersParameters.xlsx', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the x and y coordinates from degrees to metres:\n",
    "\n",
    "#def converter(df, ID):\n",
    "#    Rmars = 3390000 \n",
    "#    latc = param['central_latitude'].loc[param['HiRise_ID']==ID].values[0]\n",
    "#    lonc = param['central_longitude'].loc[param['HiRise_ID']==ID].values[0]\n",
    "#    df['x_coord_m'] = df['x_coord'].apply(lambda a:(a - latc)*Rmars*(np.pi/180))\n",
    "#    df['y_coord_m'] = df['y_coord'].apply(lambda a:(a - lonc)*Rmars*(np.pi/180)*mt.sin(mt.radians(90 - a)))\n",
    "\n",
    "#converting the test data:\n",
    "\n",
    "#converter(df1, ID1)\n",
    "#converter(df2, ID2)\n",
    "#converter(df3, ID3)\n",
    "#converter(df0, ID0)\n",
    "#print(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-edd9881a7c6f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-edd9881a7c6f>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#def find_subclustering(df,ID, eps = 11, min_samples = 3):\n",
    "    '''\n",
    "    A function to find subclusters in crater clusters by utilizing Density based Spatial Clustering.\n",
    "    The function is set up to print out a plot of the subclusters and measurements.\n",
    "    It takes as input a dataframe holding the spatial coordinates for every crater in a cluster and the HiRise ID of the associated image\n",
    "    The ID is soley needed for the title of the plot.\n",
    "    The eps gives the largest possible distance between craters to be considered part of a class\n",
    "    min_samples gives the minimum amount of craters in a class for it to be considered a cluster\n",
    "    \n",
    "    To read up on DBSCAN: https://en.wikipedia.org/wiki/DBSCAN and https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html\n",
    "    '''\n",
    "    #run DBSCAN on the clusters:\n",
    "#    X = df[['x_coord_m', 'y_coord_m']].to_numpy()\n",
    "#    db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "#    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "#    core_samples_mask[db.core_sample_indices_] = True\n",
    "#    labels = db.labels_\n",
    "#    labels_true = df.index\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "#    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "#    n_noise_ = list(labels).count(-1)\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "#    homogen = metrics.homogeneity_score(labels_true, labels)\n",
    "#    comp = metrics.completeness_score(labels_true, labels)\n",
    "#    V = metrics.v_measure_score(labels_true, labels)\n",
    "#    shil = mt.nan\n",
    "#    davies = mt.nan\n",
    "#    calinski = mt.nan\n",
    "#    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "#    print('Estimated number of noise points: %d' % n_noise_)\n",
    "#    print(\"Homogeneity: %0.3f\" % homogen)\n",
    "#    print(\"Completeness: %0.3f\" % comp)\n",
    "#    print(\"V-measure: %0.3f\" % V)\n",
    "#    if n_clusters_ >1:\n",
    "#        shil = metrics.silhouette_score(X, labels)\n",
    "#        davies  = sklearn.metrics.davies_bouldin_score(X, labels)\n",
    "#        calinski = sklearn.metrics.calinski_harabasz_score(X, labels)\n",
    "#        print(\"Silhouette Coefficient: %0.3f\"\n",
    "#              % shil)\n",
    "#        print(\"Davies Bouldin score: %0.3f\" % davies) #low equals good clustering\n",
    "#        print(\"Calinski Harabasz score: %0.3f\" % calinski) #high equals good clustering\n",
    "    # Plot result\n",
    "    # Black removed and is used for noise instead.\n",
    "#    unique_labels = set(labels)\n",
    "#    colors = [plt.cm.Spectral(each)\n",
    "#              for each in np.linspace(0, 1, len(unique_labels))]\n",
    "#    for k, col in zip(unique_labels, colors):\n",
    "#        if k == -1:\n",
    "            # Black used for noise.\n",
    "#            col = [0, 0, 0, 1]\n",
    "\n",
    "#        class_member_mask = (labels == k)\n",
    "\n",
    "#        xy = X[class_member_mask & core_samples_mask]\n",
    "#        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "#                 markeredgecolor='k', markersize=14)\n",
    "\n",
    "#        xy = X[class_member_mask & ~core_samples_mask]\n",
    "#        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "#                 markeredgecolor='k', markersize=6)\n",
    "\n",
    "#    plt.title('Subclustering in %s' %ID)\n",
    "#    plt.show()\n",
    "#    return n_clusters_, n_noise_, homogen, V, shil, davies, calinski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Running the DBSCAN for the test clusters\n",
    "#print(ID0)\n",
    "#results0 = find_subclustering(df0, ID0 )\n",
    "#print(ID1)\n",
    "#results1 = find_subclustering(df1,ID1)\n",
    "#print(ID2)\n",
    "#results2 = find_subclustering(df2,ID2)\n",
    "#print(ID3)\n",
    "#results4 = find_subclustering(df3,ID3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing and testing largest 3 clusters: (all have at least 150 craters in their cluster)\n",
    "\n",
    "#df4 = pd.read_excel('C:/Users/jae4518/OneDrive - Imperial College London/HiRise_Images_Clusters/ClustersDataSheet/ESP_046463_2160formatted.xlsx', index_col = [0,1])\n",
    "#ID4 = 'ESP_046463_2160'\n",
    "#df5 = pd.read_excel('C:/Users/jae4518/OneDrive - Imperial College London/HiRise_Images_Clusters/ClustersDataSheet/PSP_007009_1905formatted.xlsx', index_col = [0,1])\n",
    "#ID5 = 'PSP_007009_1905'\n",
    "#df6 = pd.read_excel('C:/Users/jae4518/OneDrive - Imperial College London/HiRise_Images_Clusters/ClustersDataSheet/ESP_051357_1880formatted.xlsx', index_col = [0,1])\n",
    "#ID6 = 'ESP_051357_1880'\n",
    "#converter(df4,ID4)\n",
    "#converter(df5, ID5)\n",
    "#converter(df6, ID6)\n",
    "\n",
    "#print(ID4)\n",
    "#results4 = find_subclustering(df4,ID4)\n",
    "#print(ID5)\n",
    "#results5 = find_subclustering(df5,ID5)\n",
    "#print(ID6)\n",
    "#results6 = find_subclustering(df6,ID6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all clusters with observed subclustering:\n",
    "\n",
    "#df_obs = pd.read_excel('C:/Users/jae4518/OneDrive - Imperial College London/HiRise_Images_Clusters/Analysis/NewClustersObservations.xlsx', index_col = 0)\n",
    "#df_sub = df_obs.loc[df_obs['Subclustering?'] == True]\n",
    "#sublist = df_sub['HiRise_ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running DBSCAN on the clusters with observed subclustering:\n",
    "\n",
    "#dic = {'HiRise_ID': ID, 'N_clusters':n_clusters, 'N_noise':n_noise, 'Homogeneity':homogen,\n",
    "#       'V_measure':V,'Shilhouette_Coefficient':shil, 'Davies_Bouldin_score':davies, 'Calinski_Harabasz_score': calinski}\n",
    "\n",
    "#df_resultssub = pd.DataFrame(columns=['HiRise_ID', 'N_clusters', 'N_noise', 'Homogeneity', 'V_measure','Shilhouette_Coefficient', 'Davies_Bouldin_score', 'Calinski_Harabasz_score'])\n",
    "#for ID in sublist:\n",
    "#    df = pd.read_excel('C:/Users/jae4518/OneDrive - Imperial College London/HiRise_Images_Clusters/ClustersDataSheet/%sformatted.xlsx' %ID, index_col = [0,1])\n",
    "#    print(ID)\n",
    "#    converter(df, ID)\n",
    "#    n_clusters, n_noise, homogen, V,shil, davies, calinski = find_subclustering(df, ID)\n",
    "#    df_resultssub = df_resultssub.append(dic, ignore_index = True) #adding the results to new results dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding all craters with no observed subclustering:\n",
    "\n",
    "#df_nosub = df_obs.loc[df_obs['Subclustering?'] == False]\n",
    "#nolist = df_nosub['HiRise_ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#running DBSCAN on clusters without observed clustering\n",
    "\n",
    "#df_resultsnosub = pd.DataFrame(columns=['HiRise_ID', 'N_clusters', 'N_noise', 'Homogeneity', 'V_measure','Shilhouette_Coefficient', 'Davies_Bouldin_score', 'Calinski_Harabasz_score'])\n",
    "#for ID in nolist:\n",
    "#    df = pd.read_excel('C:/Users/jae4518/OneDrive - Imperial College London/HiRise_Images_Clusters/ClustersDataSheet/%sformatted.xlsx' %ID, index_col = [0,1])\n",
    "#    print(ID)\n",
    "#    converter(df, ID)\n",
    "#    n_clusters, n_noise, homogen, V,shil, davies, calinski = find_subclustering(df,ID)\n",
    "#   df_resultsnosub = df_resultsnosub.append(dic, ignore_index = True) #adding results to new results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Calinski Harabasz score distribution for observed and not observed \n",
    "\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111)\n",
    "#ax.hist(df_resultssub['Calinski_Harabasz_score'], 10, density = True,alpha = 0.5, label = 'Observed subclustering')\n",
    "#ax.hist(df_resultsnosub['Calinski_Harabasz_score'], 10,density = True,alpha = 0.5, label = 'No observed subclustering')\n",
    "#ax.legend(loc = 'best')\n",
    "#ax.set_title('Calinski Harabasz score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Davies Bouldin score histogram\n",
    "\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111)\n",
    "#ax.hist(df_resultssub['Davies_Bouldin_score'], 10,density = True, alpha = 0.5, label = 'Observed subclustering')\n",
    "#ax.hist(df_resultsnosub['Davies_Bouldin_score'], 10,density = True,alpha = 0.5, label = 'No observed subclustering')\n",
    "#ax.legend(loc = 'best')\n",
    "#ax.set_title('Davies_Bouldin_score score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting V measure histogram\n",
    "\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111)\n",
    "#ax.hist(df_resultssub['V_measure'], 10,density = True, label = 'Observed subclustering', alpha = 0.5)\n",
    "#ax.hist(df_resultsnosub['V_measure'], 10,density = True, label = 'No observed subclustering', alpha = 0.5)\n",
    "#ax.legend(loc = 'best')\n",
    "#ax.set_title('V measure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
